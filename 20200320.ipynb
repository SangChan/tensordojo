{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# 64개의 유닛을 가진 완전 연결 층을 모델에 추가합니다:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# 또 하나를 추가합니다:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# 10개의 출력 유닛을 가진 소프트맥스 층을 추가합니다:\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 활성화 층을 만듭니다:\n",
    "layers.Dense(64, activation='sigmoid')\n",
    "# 또는 다음도 가능합니다:\n",
    "layers.Dense(64, activation=tf.keras.activations.sigmoid)\n",
    "\n",
    "# 커널 행렬에 L1 규제가 적용된 선형 활성화 층. 하이퍼파라미터 0.01은 규제의 양을 조절합니다:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
    "\n",
    "# 절편 벡터에 L2 규제가 적용된 선형 활성화 층. 하이퍼파라미터 0.01은 규제의 양을 조절합니다:\n",
    "layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "# 커널을 랜덤한 직교 행렬로 초기화한 선형 활성화 층:\n",
    "layers.Dense(64, kernel_initializer='orthogonal')\n",
    "\n",
    "# 절편 벡터를 상수 2.0으로 설정한 선형 활성화 층:\n",
    "layers.Dense(64, bias_initializer=tf.keras.initializers.Constant(2.0))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "# 64개의 유닛을 가진 완전 연결 층을 모델에 추가합니다:\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "# 또 하나를 추가합니다:\n",
    "layers.Dense(64, activation='relu'),\n",
    "# 10개의 출력 유닛을 가진 소프트맥스 층을 추가합니다:\n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "# 평균 제곱 오차로 회귀 모델을 설정합니다.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='mse',       # 평균 제곱 오차\n",
    "              metrics=['mae'])  # 평균 절댓값 오차\n",
    "\n",
    "# 크로스엔트로피 손실 함수로 분류 모델을 설정합니다.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 280.0192 - categorical_accuracy: 0.0910 - val_loss: 634.8644 - val_categorical_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 1109.8808 - categorical_accuracy: 0.1030 - val_loss: 1298.8079 - val_categorical_accuracy: 0.0800\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 2259.6161 - categorical_accuracy: 0.0960 - val_loss: 3536.0444 - val_categorical_accuracy: 0.1300\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 176us/sample - loss: 3831.1100 - categorical_accuracy: 0.0970 - val_loss: 6054.9377 - val_categorical_accuracy: 0.0600\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 5630.0098 - categorical_accuracy: 0.1150 - val_loss: 5311.6862 - val_categorical_accuracy: 0.1700\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 7995.3881 - categorical_accuracy: 0.1000 - val_loss: 9584.7055 - val_categorical_accuracy: 0.1300\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 10359.0947 - categorical_accuracy: 0.1120 - val_loss: 7755.8333 - val_categorical_accuracy: 0.0700\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 165us/sample - loss: 12995.2503 - categorical_accuracy: 0.0940 - val_loss: 12619.6056 - val_categorical_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 16891.9724 - categorical_accuracy: 0.0900 - val_loss: 9613.8116 - val_categorical_accuracy: 0.1200\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 19934.1254 - categorical_accuracy: 0.1120 - val_loss: 15392.4071 - val_categorical_accuracy: 0.1700\n",
      "Train for 30 steps\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 23304.6532 - categorical_accuracy: 0.0927\n",
      "Epoch 2/10\n",
      " 1/30 [>.............................] - ETA: 0s - loss: 18686.6875 - categorical_accuracy: 0.1562WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
      " 2/30 [=>............................] - ETA: 2s - loss: 21738.4004 - categorical_accuracy: 0.1250Epoch 1/10\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 28211.0573 - categorical_accuracy: 0.1060 - val_loss: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 32499.8935 - categorical_accuracy: 0.0990 - val_loss: 30346.8330 - val_categorical_accuracy: 0.0800\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 36429.1260 - categorical_accuracy: 0.0970 - val_loss: 32629.3350 - val_categorical_accuracy: 0.0600\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 41321.5541 - categorical_accuracy: 0.1040 - val_loss: 43973.1797 - val_categorical_accuracy: 0.0600\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 45526.3913 - categorical_accuracy: 0.1170 - val_loss: 60868.9277 - val_categorical_accuracy: 0.0700\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 52533.3395 - categorical_accuracy: 0.1140 - val_loss: 87259.2539 - val_categorical_accuracy: 0.0700\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 59711.1424 - categorical_accuracy: 0.1070 - val_loss: 69788.7793 - val_categorical_accuracy: 0.0600\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 63852.6131 - categorical_accuracy: 0.1240 - val_loss: 84560.4160 - val_categorical_accuracy: 0.0600\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 72205.6805 - categorical_accuracy: 0.1030 - val_loss: 107614.7520 - val_categorical_accuracy: 0.0700\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 80023.2739 - categorical_accuracy: 0.0940 - val_loss: 87484.6582 - val_categorical_accuracy: 0.0800\n",
      "1000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 81us/sample - loss: 83838.4978 - categorical_accuracy: 0.1070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step - loss: 83986.4849 - categorical_accuracy: 0.1010\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))\n",
    "\n",
    "# 예제 `Dataset` 객체를 만듭니다:\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "# Dataset에서 `fit` 메서드를 호출할 때 `steps_per_epoch` 설정을 잊지 마세요.\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "\n",
    "model.fit(dataset, epochs=10,\n",
    "          validation_data=val_dataset)\n",
    "          \n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "model.evaluate(dataset, steps=30)\n",
    "\n",
    "result = model.predict(data, batch_size=32)\n",
    "print(result.shape)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,))  # 입력 플레이스홀더를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 12.7398 - accuracy: 0.1090\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 17.8893 - accuracy: 0.1130\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 28.1110 - accuracy: 0.1010\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 41.1158 - accuracy: 0.1010\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 142us/sample - loss: 52.6753 - accuracy: 0.0970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe031afcc18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 층 객체는 텐서를 사용하여 호출되고 텐서를 반환합니다.\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# 컴파일 단계는 훈련 과정을 설정합니다.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5번의 에포크 동안 훈련합니다.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 11.5631 - accuracy: 0.0990\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 11.5454 - accuracy: 0.1180\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 11.5406 - accuracy: 0.1060\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 11.5373 - accuracy: 0.1120\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 113us/sample - loss: 11.5345 - accuracy: 0.1120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe038600cf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(MyModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # 층을 정의합니다.\n",
    "    self.dense_1 = layers.Dense(32, activation='relu')\n",
    "    self.dense_2 = layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # 정방향 패스를 정의합니다.\n",
    "    # `__init__` 메서드에서 정의한 층을 사용합니다.\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# 컴파일 단계는 훈련 과정을 설정합니다.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5번의 에포크 동안 훈련합니다.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 814us/sample - loss: 11.5482 - accuracy: 0.1140\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 11.5481 - accuracy: 0.1150\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 11.5480 - accuracy: 0.1120\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 99us/sample - loss: 11.5477 - accuracy: 0.1110\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 11.5472 - accuracy: 0.1110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0381ca898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    self.output_dim = output_dim\n",
    "    super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # 이 층에서 훈련할 가중치 변수를 만듭니다.\n",
    "    self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=(input_shape[1], self.output_dim),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super(MyLayer, self).get_config()\n",
    "    base_config['output_dim'] = self.output_dim\n",
    "    return base_config\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    MyLayer(10),\n",
    "    layers.Activation('softmax')])\n",
    "\n",
    "# 컴파일 단계는 훈련 과정을 설정합니다.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5번의 에포크 동안 훈련합니다.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 350us/sample - loss: 11.5476 - accuracy: 0.1110 - val_loss: 11.3470 - val_accuracy: 0.0800\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 122us/sample - loss: 11.5475 - accuracy: 0.1140 - val_loss: 11.3464 - val_accuracy: 0.0800\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 11.5473 - accuracy: 0.1120 - val_loss: 11.3456 - val_accuracy: 0.0800\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 11.5473 - accuracy: 0.1150 - val_loss: 11.3469 - val_accuracy: 0.0700\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 121us/sample - loss: 11.5474 - accuracy: 0.1140 - val_loss: 11.3466 - val_accuracy: 0.0800\n",
      "{'backend': 'tensorflow',\n",
      " 'class_name': 'Sequential',\n",
      " 'config': {'layers': [{'class_name': 'Dense',\n",
      "                        'config': {'activation': 'relu',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'batch_input_shape': [None, 32],\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_17',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 64,\n",
      "                                   'use_bias': True}},\n",
      "                       {'class_name': 'Dense',\n",
      "                        'config': {'activation': 'softmax',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float32',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_18',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 10,\n",
      "                                   'use_bias': True}}],\n",
      "            'name': 'sequential_3'},\n",
      " 'keras_version': '2.2.4-tf'}\n",
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "  layers:\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: relu\n",
      "      activity_regularizer: null\n",
      "      batch_input_shape: !!python/tuple\n",
      "      - null\n",
      "      - 32\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_17\n",
      "      trainable: true\n",
      "      units: 64\n",
      "      use_bias: true\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: softmax\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float32\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_18\n",
      "      trainable: true\n",
      "      units: 10\n",
      "      use_bias: true\n",
      "  name: sequential_3\n",
      "keras_version: 2.2.4-tf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 11.5438 - accuracy: 0.0960\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 11.5511 - accuracy: 0.1090\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 11.5578 - accuracy: 0.1220\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 104us/sample - loss: 11.5597 - accuracy: 0.1040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 105us/sample - loss: 11.5771 - accuracy: 0.0980\n",
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.6969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe02b6a4ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "  # `val_loss`가 2번의 에포크에 걸쳐 향상되지 않으면 훈련을 멈춥니다.\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # `./logs` 디렉토리에 텐서보드 로그를 기록니다.\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(val_data, val_labels))\n",
    "          \n",
    "model = tf.keras.Sequential([\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 가중치를 텐서플로의 체크포인트 파일로 저장합니다.\n",
    "model.save_weights('./weights/my_model')\n",
    "\n",
    "# 모델의 상태를 복원합니다.\n",
    "# 모델의 구조가 동일해야 합니다.\n",
    "model.load_weights('./weights/my_model')\n",
    "\n",
    "# 가중치를 HDF5 파일로 저장합니다.\n",
    "model.save_weights('my_model.h5', save_format='h5')\n",
    "\n",
    "# 모델의 상태를 복원합니다.\n",
    "model.load_weights('my_model.h5')\n",
    "\n",
    "# 모델을 JSON 포맷으로 직렬화합니다.\n",
    "json_string = model.to_json()\n",
    "json_string\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "pprint.pprint(json.loads(json_string))\n",
    "\n",
    "fresh_model = tf.keras.models.model_from_json(json_string)\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "print(yaml_string)\n",
    "\n",
    "fresh_model = tf.keras.models.model_from_yaml(yaml_string)\n",
    "\n",
    "# 간단한 모델을 만듭니다.\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(10, activation='softmax', input_shape=(32,)),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, batch_size=32, epochs=5)\n",
    "\n",
    "\n",
    "# 전체 모델을 HDF5 파일로 저장합니다.\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# 가중치와 옵티마이저를 포함하여 정확히 같은 모델을 다시 만듭니다.\n",
    "model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "  model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.SGD(0.2)\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "x = np.random.random((1024, 10))\n",
    "y = np.random.randint(2, size=(1024, 1))\n",
    "x = tf.cast(x, tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
