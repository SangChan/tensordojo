{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 SimpleRNN 레이어 생성 코드\n",
    "rnn1 = tf.keras.layers.SimpleRNN(units=1, activation='tanh', return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. ]\n",
      " [0.1]\n",
      " [0.2]\n",
      " [0.3]] 0.4\n",
      "[[0.1]\n",
      " [0.2]\n",
      " [0.3]\n",
      " [0.4]] 0.5\n",
      "[[0.2]\n",
      " [0.3]\n",
      " [0.4]\n",
      " [0.5]] 0.6\n",
      "[[0.3]\n",
      " [0.4]\n",
      " [0.5]\n",
      " [0.6]] 0.7\n",
      "[[0.4]\n",
      " [0.5]\n",
      " [0.6]\n",
      " [0.7]] 0.8\n",
      "[[0.5]\n",
      " [0.6]\n",
      " [0.7]\n",
      " [0.8]] 0.9\n"
     ]
    }
   ],
   "source": [
    "# 7.2 시퀀스 예측 데이터 생성\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(6):\n",
    "    # [0,1,2,3], [1,2,3,4] 같은 정수의 시퀀스를 만듭니다.\n",
    "    lst = list(range(i,i+4))\n",
    "\n",
    "    # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장합니다.\n",
    "    # SimpleRNN 에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장합니다.\n",
    "    X.append(list(map(lambda c: [c/10], lst)))\n",
    "\n",
    "    # 정답에 해당하는 4, 5 등의 정수를 역시 위처럼 10으로 나눠서 저장합니다.\n",
    "    Y.append((i+4)/10)\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "for i in range(len(X)):\n",
    "    print(X[i], Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.3 시퀀스 예측 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3556897 ]\n",
      " [0.49260497]\n",
      " [0.61480206]\n",
      " [0.71949244]\n",
      " [0.80600584]\n",
      " [0.8751545 ]]\n"
     ]
    }
   ],
   "source": [
    "# 7.4 네트워크 훈련 및 결과 확인\n",
    "model.fit(X, Y, epochs=100, verbose=0)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92856807]]\n",
      "[[0.20918939]]\n"
     ]
    }
   ],
   "source": [
    "# 7.5 학습되지 않은 시퀀스에 대한 예측 결과\n",
    "print(model.predict(np.array([[[0.6],[0.7],[0.8],[0.9]]])))\n",
    "print(model.predict(np.array([[[-0.1],[0.0],[0.1],[0.2]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57782846]\n",
      " [0.         0.06927981]\n",
      " [0.         0.24218327]\n",
      " [0.         0.95859191]\n",
      " [0.         0.90753018]\n",
      " [0.         0.67973782]\n",
      " [0.         0.98894957]\n",
      " [0.         0.15830201]\n",
      " [0.         0.47791069]\n",
      " [0.         0.83229434]\n",
      " [0.         0.49483914]\n",
      " [0.         0.30377009]\n",
      " [0.         0.63117398]\n",
      " [0.         0.72439188]\n",
      " [0.         0.86075122]\n",
      " [0.         0.35100617]\n",
      " [0.         0.91704571]\n",
      " [0.         0.48874659]\n",
      " [0.         0.07207691]\n",
      " [0.         0.84446778]\n",
      " [0.         0.07199783]\n",
      " [0.         0.76520103]\n",
      " [0.         0.57682067]\n",
      " [0.         0.5553233 ]\n",
      " [1.         0.87098905]\n",
      " [0.         0.78312747]\n",
      " [0.         0.1790017 ]\n",
      " [0.         0.31255667]\n",
      " [0.         0.10707913]\n",
      " [1.         0.43821367]\n",
      " [0.         0.57969402]\n",
      " [0.         0.28658349]\n",
      " [0.         0.98837234]\n",
      " [0.         0.50533892]\n",
      " [0.         0.08686632]\n",
      " [0.         0.82858727]\n",
      " [0.         0.7056291 ]\n",
      " [0.         0.04052008]\n",
      " [0.         0.47327185]\n",
      " [0.         0.21516657]\n",
      " [0.         0.03763662]\n",
      " [0.         0.36207631]\n",
      " [0.         0.43136897]\n",
      " [0.         0.52238365]\n",
      " [0.         0.54069465]\n",
      " [0.         0.28093218]\n",
      " [0.         0.46523955]\n",
      " [0.         0.69516723]\n",
      " [0.         0.38753118]\n",
      " [0.         0.3087048 ]\n",
      " [0.         0.90925526]\n",
      " [0.         0.61884781]\n",
      " [0.         0.90390034]\n",
      " [0.         0.10121581]\n",
      " [0.         0.88725515]\n",
      " [0.         0.28391852]\n",
      " [0.         0.60317323]\n",
      " [0.         0.17119711]\n",
      " [0.         0.07055195]\n",
      " [0.         0.47417037]\n",
      " [0.         0.98063194]\n",
      " [0.         0.29534509]\n",
      " [0.         0.63937795]\n",
      " [0.         0.47732069]\n",
      " [0.         0.57658886]\n",
      " [0.         0.98138554]\n",
      " [0.         0.11676599]\n",
      " [0.         0.74485196]\n",
      " [0.         0.60939974]\n",
      " [0.         0.97100447]\n",
      " [0.         0.72437082]\n",
      " [0.         0.90474266]\n",
      " [0.         0.26660834]\n",
      " [0.         0.18361896]\n",
      " [0.         0.60838334]\n",
      " [0.         0.93531778]\n",
      " [0.         0.58601902]\n",
      " [0.         0.83141654]\n",
      " [0.         0.25231808]\n",
      " [0.         0.02708873]\n",
      " [0.         0.4588013 ]\n",
      " [0.         0.5057237 ]\n",
      " [0.         0.516431  ]\n",
      " [0.         0.68804479]\n",
      " [0.         0.44297795]\n",
      " [0.         0.24752973]\n",
      " [0.         0.79938079]\n",
      " [0.         0.98226619]\n",
      " [0.         0.45551028]\n",
      " [0.         0.91508206]\n",
      " [0.         0.9484176 ]\n",
      " [0.         0.49270706]\n",
      " [0.         0.27563897]\n",
      " [0.         0.91065652]\n",
      " [0.         0.33902628]\n",
      " [0.         0.15079316]\n",
      " [0.         0.53319491]\n",
      " [0.         0.54205604]\n",
      " [0.         0.08271943]\n",
      " [0.         0.37811827]] 0.38167930987389737\n"
     ]
    }
   ],
   "source": [
    "# 7.6 곱셈 문제 데이터 생성\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(3000):\n",
    "    # 0~1 사이의 랜덤한 숫자 100 개를 만듭니다.\n",
    "    lst = np.random.rand(100)\n",
    "    # 마킹할 숫자 2개의 인덱스를 뽑습니다.\n",
    "    idx = np.random.choice(100, 2, replace=False)\n",
    "    # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다.\n",
    "    zeros = np.zeros(100)\n",
    "    zeros[idx] = 1\n",
    "    # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X 에 저장합니다.\n",
    "    X.append(np.array(list(zip(zeros, lst))))\n",
    "    # 마킹 인덱스가 1인 값들만 서로 곱해서 Y 에 저장합니다.\n",
    "    Y.append(np.prod(lst[idx]))\n",
    "    \n",
    "print(X[0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 100, 30)           990       \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 2,851\n",
      "Trainable params: 2,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.7 SimpleRNN 레이어를 사용한 곱셈 문제 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]),\n",
    "    tf.keras.layers.SimpleRNN(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 512 samples\n",
      "Epoch 1/100\n",
      "2048/2048 [==============================] - 10s 5ms/sample - loss: 0.0630 - val_loss: 0.0421\n",
      "Epoch 2/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0473 - val_loss: 0.0415\n",
      "Epoch 3/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0471 - val_loss: 0.0439\n",
      "Epoch 4/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0474 - val_loss: 0.0427\n",
      "Epoch 5/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0469 - val_loss: 0.0626\n",
      "Epoch 6/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0490 - val_loss: 0.0417\n",
      "Epoch 7/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0467 - val_loss: 0.0438\n",
      "Epoch 8/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0465 - val_loss: 0.0445\n",
      "Epoch 9/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0479 - val_loss: 0.0437\n",
      "Epoch 10/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0486 - val_loss: 0.0475\n",
      "Epoch 11/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0490 - val_loss: 0.0424\n",
      "Epoch 12/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0466 - val_loss: 0.0424\n",
      "Epoch 13/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0466 - val_loss: 0.0419\n",
      "Epoch 14/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0463 - val_loss: 0.0446\n",
      "Epoch 15/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0470 - val_loss: 0.0419\n",
      "Epoch 16/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0468 - val_loss: 0.0419\n",
      "Epoch 17/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0463 - val_loss: 0.0429\n",
      "Epoch 18/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0475 - val_loss: 0.0418\n",
      "Epoch 19/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0461 - val_loss: 0.0418\n",
      "Epoch 20/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0471 - val_loss: 0.0447\n",
      "Epoch 21/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0471 - val_loss: 0.0420\n",
      "Epoch 22/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0475 - val_loss: 0.0432\n",
      "Epoch 23/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0470 - val_loss: 0.0438\n",
      "Epoch 24/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0463 - val_loss: 0.0422\n",
      "Epoch 25/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0467 - val_loss: 0.0502\n",
      "Epoch 26/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0461 - val_loss: 0.0420\n",
      "Epoch 27/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0461 - val_loss: 0.0435\n",
      "Epoch 28/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0456 - val_loss: 0.0424\n",
      "Epoch 29/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0465 - val_loss: 0.0422\n",
      "Epoch 30/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0465 - val_loss: 0.0432\n",
      "Epoch 31/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0458 - val_loss: 0.0430\n",
      "Epoch 32/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0454 - val_loss: 0.0429\n",
      "Epoch 33/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0453 - val_loss: 0.0444\n",
      "Epoch 34/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0459 - val_loss: 0.0431\n",
      "Epoch 35/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0454 - val_loss: 0.0429\n",
      "Epoch 36/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0459 - val_loss: 0.0434\n",
      "Epoch 37/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0461 - val_loss: 0.0488\n",
      "Epoch 38/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0454 - val_loss: 0.0418\n",
      "Epoch 39/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0464 - val_loss: 0.0430\n",
      "Epoch 40/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0455 - val_loss: 0.0418\n",
      "Epoch 41/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0441 - val_loss: 0.0440\n",
      "Epoch 42/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0443 - val_loss: 0.0443\n",
      "Epoch 43/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0441 - val_loss: 0.0416\n",
      "Epoch 44/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0446 - val_loss: 0.0495\n",
      "Epoch 45/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0440 - val_loss: 0.0462\n",
      "Epoch 46/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0443 - val_loss: 0.0434\n",
      "Epoch 47/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0436 - val_loss: 0.0422\n",
      "Epoch 48/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0439 - val_loss: 0.0439\n",
      "Epoch 49/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0429 - val_loss: 0.0431\n",
      "Epoch 50/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0426 - val_loss: 0.0433\n",
      "Epoch 51/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0420 - val_loss: 0.0462\n",
      "Epoch 52/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0413 - val_loss: 0.0438\n",
      "Epoch 53/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0429 - val_loss: 0.0441\n",
      "Epoch 54/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0414 - val_loss: 0.0454\n",
      "Epoch 55/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0416 - val_loss: 0.0448\n",
      "Epoch 56/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0412 - val_loss: 0.0447\n",
      "Epoch 57/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0408 - val_loss: 0.0462\n",
      "Epoch 58/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0399 - val_loss: 0.0456\n",
      "Epoch 59/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0403 - val_loss: 0.0440\n",
      "Epoch 60/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0415 - val_loss: 0.0445\n",
      "Epoch 61/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0396 - val_loss: 0.0476\n",
      "Epoch 62/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0415 - val_loss: 0.0444\n",
      "Epoch 63/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0393 - val_loss: 0.0443\n",
      "Epoch 64/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0390 - val_loss: 0.0466\n",
      "Epoch 65/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0390 - val_loss: 0.0458\n",
      "Epoch 66/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0386 - val_loss: 0.0456\n",
      "Epoch 67/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0380 - val_loss: 0.0473\n",
      "Epoch 68/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0376 - val_loss: 0.0464\n",
      "Epoch 69/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0373 - val_loss: 0.0507\n",
      "Epoch 70/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0375 - val_loss: 0.0460\n",
      "Epoch 71/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0374 - val_loss: 0.0464\n",
      "Epoch 72/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0375 - val_loss: 0.0469\n",
      "Epoch 73/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0368 - val_loss: 0.0479\n",
      "Epoch 74/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0355 - val_loss: 0.0485\n",
      "Epoch 75/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0353 - val_loss: 0.0501\n",
      "Epoch 76/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0360 - val_loss: 0.0478\n",
      "Epoch 77/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0345 - val_loss: 0.0486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0352 - val_loss: 0.0518\n",
      "Epoch 79/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0343 - val_loss: 0.0480\n",
      "Epoch 80/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0337 - val_loss: 0.0506\n",
      "Epoch 81/100\n",
      "2048/2048 [==============================] - 5s 3ms/sample - loss: 0.0347 - val_loss: 0.0480\n",
      "Epoch 82/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0331 - val_loss: 0.0515\n",
      "Epoch 83/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0331 - val_loss: 0.0538\n",
      "Epoch 84/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0334 - val_loss: 0.0528\n",
      "Epoch 85/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0328 - val_loss: 0.0489\n",
      "Epoch 86/100\n",
      "2048/2048 [==============================] - 6s 3ms/sample - loss: 0.0328 - val_loss: 0.0509\n",
      "Epoch 87/100\n",
      "1792/2048 [=========================>....] - ETA: 0s - loss: 0.0319"
     ]
    }
   ],
   "source": [
    "# 7.8 SimpleRNN 네트워크 학습\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# 2560개의 데이터만 학습시킵니다. validation 데이터는 20% 로 지정합니다.\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.9 SimpleRNN 네트워크 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.10 Test 데이터에 대한 예측 정확도 확인\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560+5])\n",
    "# 5개 테스트 데이터에 대한 예측을 표시합니다.\n",
    "for i in range(5):\n",
    "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
    "    \n",
    "prediction = model.predict(X[2560:])\n",
    "fail = 0\n",
    "for i in range(len(prediction)):\n",
    "    # 오차가 0.04 이상이면 오답입니다.\n",
    "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
    "        fail += 1\n",
    "print('correctness:', (440 - fail) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.11 LSTM 레이어를 사용한 곱셈 문제 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]),\n",
    "    tf.keras.layers.LSTM(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.12 LSTM 네트워크 학습\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.13 LSTM 네트워크 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.14 Test 데이터에 대한 예측 정확도 확인\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560+5])\n",
    "for i in range(5):\n",
    "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
    "    \n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
    "        cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.15 GRU 레이어를 사용한 곱셈 문제 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(units=30, return_sequences=True, input_shape=[100,2]),\n",
    "    tf.keras.layers.GRU(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.16 GRU 네트워크 학습\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.17 GRU 네트워크 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.18 Test 데이터에 대한 예측 정확도 확인\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560+5])\n",
    "for i in range(5):\n",
    "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
    "    \n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
    "        cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.19 Naver Sentiment Movie Corpus v1.0 다운로드\n",
    "path_to_train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "path_to_test_file = tf.keras.utils.get_file('test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 6937271 characters\n",
      "Length of text: 2318260 characters\n",
      "\n",
      "id\tdocument\tlabel\n",
      "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
      "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
      "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
      "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
      "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
      "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
      "7797314\t원작의\n"
     ]
    }
   ],
   "source": [
    "# 7.20 데이터 로드 및 확인\n",
    "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
    "train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\n",
    "test_text = open(path_to_test_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print('Length of text: {} characters'.format(len(test_text)))\n",
    "print()\n",
    "\n",
    "# 처음 300 자를 확인해봅니다.\n",
    "print(train_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 1) (50000, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 7.21 학습을 위한 정답 데이터(Y) 만들기\n",
    "train_Y = np.array([[int(row.split('\\t')[2])] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
    "test_Y = np.array([[int(row.split('\\t')[2])] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
    "print(train_Y.shape, test_Y.shape)\n",
    "print(train_Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '진짜', '짜증나네요', '목소리']\n",
      "['흠', '포스터보고', '초딩영화줄', '오버연기조차', '가볍지', '않구나']\n",
      "['너무재밓었다그래서보는것을추천한다']\n",
      "['교도소', '이야기구먼', '솔직히', '재미는', '없다', '평점', '조정']\n",
      "['사이몬페그의', '익살스런', '연기가', '돋보였던', '영화', '!', '스파이더맨에서', '늙어보이기만', '했던', '커스틴', '던스트가', '너무나도', '이뻐보였다']\n"
     ]
    }
   ],
   "source": [
    "# 7.22 train 데이터의 입력(X)에 대한 정제(Cleaning)\n",
    "import re\n",
    "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):    \n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "\n",
    "    return string.lower()\n",
    "\n",
    "\n",
    "train_text_X = [row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
    "train_text_X = [clean_str(sentence) for sentence in train_text_X]\n",
    "# 문장을 띄어쓰기 단위로 단어 분리\n",
    "sentences = [sentence.split(' ') for sentence in train_text_X]\n",
    "for i in range(5):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142587\n"
     ]
    }
   ],
   "source": [
    "# 7.23 각 문장의 단어 길이 확인\n",
    "import matplotlib.pyplot as plt\n",
    "sentence_len = [len(sentence) for sentence in sentences]\n",
    "sentence_len.sort()\n",
    "plt.plot(sentence_len)\n",
    "plt.show()\n",
    "\n",
    "print(sum([int(l<=25) for l in sentence_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '진짜', '짜증나네요', '목소리']\n",
      "['흠', '포스터보고', '초딩영화줄', '오버연기조', '가볍지', '않구나']\n",
      "['너무재밓었']\n",
      "['교도소', '이야기구먼', '솔직히', '재미는', '없다', '평점', '조정']\n",
      "['사이몬페그', '익살스런', '연기가', '돋보였던', '영화', '!', '스파이더맨', '늙어보이기', '했던', '커스틴', '던스트가', '너무나도', '이뻐보였다']\n"
     ]
    }
   ],
   "source": [
    "# 7.24 단어 정제 및 문장 길이 줄임\n",
    "sentences_new = []\n",
    "for sentence in sentences:\n",
    "    sentences_new.append([word[:5] for word in sentence][:25])\n",
    "sentences = sentences_new\n",
    "for i in range(5):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   25   884     8  5795  1111     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [  588  5796  6697     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [   71   346    31    35 10468     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [  106  5338     4     2  2169   869   573     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]]\n"
     ]
    }
   ],
   "source": [
    "# 7.25 Tokenizer와 pad_sequences를 사용한 문장 전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "train_X = tokenizer.texts_to_sequences(sentences)\n",
    "train_X = pad_sequences(train_X, padding='post')\n",
    "\n",
    "print(train_X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경우는\n",
      "잊혀질\n",
      "[[], [19999], [], [106]]\n",
      "[[    0]\n",
      " [19999]\n",
      " [    0]\n",
      " [  106]]\n"
     ]
    }
   ],
   "source": [
    "# 7.26 Tokenizer의 동작 확인\n",
    "print(tokenizer.index_word[19999])\n",
    "print(tokenizer.index_word[20000])\n",
    "temp = tokenizer.texts_to_sequences(['#$#$#', '경우는', '잊혀질', '연기가'])\n",
    "print(temp)\n",
    "temp = pad_sequences(temp, padding='post')\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           6000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,070,302\n",
      "Trainable params: 6,070,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.27 감성 분석을 위한 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(20000, 300, input_length=25),\n",
    "    tf.keras.layers.LSTM(units=50),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/3\n",
      "120000/120000 [==============================] - 520s 4ms/sample - loss: 0.4344 - accuracy: 0.7839 - val_loss: 0.3767 - val_accuracy: 0.8215\n",
      "Epoch 2/3\n",
      "120000/120000 [==============================] - 514s 4ms/sample - loss: 0.3238 - accuracy: 0.8478 - val_loss: 0.3926 - val_accuracy: 0.8187\n",
      "Epoch 3/3\n",
      " 60032/120000 [==============>...............] - ETA: 3:58 - loss: 0.2638 - accuracy: 0.8741"
     ]
    }
   ],
   "source": [
    "# 7.28 감성 분석 모델 학습\n",
    "history = model.fit(train_X, train_Y, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.29 감성 분석 모델 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.30 테스트 데이터 평가\n",
    "test_text_X = [row.split('\\t')[1] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
    "test_text_X = [clean_str(sentence) for sentence in test_text_X]\n",
    "sentences = [sentence.split(' ') for sentence in test_text_X]\n",
    "sentences_new = []\n",
    "for sentence in sentences:\n",
    "    sentences_new.append([word[:5] for word in sentence][:25])\n",
    "sentences = sentences_new\n",
    "\n",
    "test_X = tokenizer.texts_to_sequences(sentences)\n",
    "test_X = pad_sequences(test_X, padding='post')\n",
    "\n",
    "model.evaluate(test_X, test_Y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.31 임의의 문장 감성 분석 결과 확인\n",
    "test_sentence = '재미있을 줄 알았는데 완전 실망했다. 너무 졸리고 돈이 아까웠다.'\n",
    "test_sentence = test_sentence.split(' ')\n",
    "test_sentences = []\n",
    "now_sentence = []\n",
    "for word in test_sentence:\n",
    "    now_sentence.append(word)\n",
    "    test_sentences.append(now_sentence[:])\n",
    "    \n",
    "test_X_1 = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_X_1 = pad_sequences(test_X_1, padding='post', maxlen=25)\n",
    "prediction = model.predict(test_X_1)\n",
    "for idx, sentence in enumerate(test_sentences):\n",
    "    print(sentence)\n",
    "    print(prediction[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://bit.ly/2Mc3SOV\n",
      "62013440/62012502 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 7.32 조선왕조실록 데이터 파일 다운로드\n",
    "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 26265493 characters\n",
      "\n",
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
     ]
    }
   ],
   "source": [
    "# 7.33 데이터 로드 및 확인\n",
    "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print()\n",
    "\n",
    "# 처음 100 자를 확인해봅니다.\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
     ]
    }
   ],
   "source": [
    "# 7.34 훈련 데이터 입력 정제\n",
    "import re\n",
    "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):    \n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "train_text = train_text.split('\\n')\n",
    "train_text = [clean_str(sentence) for sentence in train_text]\n",
    "train_text_X = []\n",
    "for sentence in train_text:\n",
    "    train_text_X.extend(sentence.split(' '))\n",
    "    train_text_X.append('\\n')\n",
    "    \n",
    "train_text_X = [word for word in train_text_X if word != '']\n",
    "\n",
    "print(train_text_X[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332640 unique words\n",
      "{\n",
      "  '\\n':   0,\n",
      "  '!' :   1,\n",
      "  ',' :   2,\n",
      "  '000명으로':   3,\n",
      "  '001':   4,\n",
      "  '002':   5,\n",
      "  '003':   6,\n",
      "  '004':   7,\n",
      "  '005':   8,\n",
      "  '006':   9,\n",
      "  ...\n",
      "}\n",
      "index of UNK: 332639\n"
     ]
    }
   ],
   "source": [
    "# 7.35 단어 토큰화\n",
    "# 단어의 set을 만듭니다.\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print ('{} unique words'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
    "print('{')\n",
    "for word,_ in zip(word2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(word2idx['UNK']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]\n"
     ]
    }
   ],
   "source": [
    "# 7.36 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2 330313]\n"
     ]
    }
   ],
   "source": [
    "# 7.37 기본 데이터셋 만들기\n",
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2]\n",
      "휘\n",
      "330313\n"
     ]
    }
   ],
   "source": [
    "# 7.38 학습 데이터셋 만들기\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "for x,y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.39 데이터셋 shuffle, batch 설정\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 100)           33264000  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 332640)            33596640  \n",
      "=================================================================\n",
      "Total params: 67,021,440\n",
      "Trainable params: 67,021,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.40 단어 단위 생성 모델 정의\n",
    "total_words = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 533 steps\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# 7.41 단어 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = train_text[0]\n",
    "\n",
    "    next_words = 100\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "    \n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.42 임의의 문장을 사용한 생성 결과 확인\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
    "\n",
    "next_words = 100\n",
    "for _ in range(next_words):\n",
    "    test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "    \n",
    "    output_idx = model.predict_classes(test_text_X)\n",
    "    test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "\n",
    "print(test_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
